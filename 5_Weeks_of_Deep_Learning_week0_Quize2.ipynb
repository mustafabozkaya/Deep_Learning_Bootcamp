{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO7T5XUrLjgxh+DkKbQJKF9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mustafabozkaya/Deep_Learning_Bootcamp/blob/master/5_Weeks_of_Deep_Learning_week0_Quize2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.\n",
        "Select the correct option\n",
        "The hyperparameters of neural networks is/are:\n",
        "\n",
        "1/1 XP\n",
        "\n",
        "\n",
        "Learning Rate\n",
        "\n",
        "\n",
        "Epochs\n",
        "\n",
        "\n",
        "Batch Size\n",
        "\n",
        "\n",
        "All of the above are the hyperparameters of neural networks\n",
        "\n",
        "Your answer\n",
        "\n",
        "2.\n",
        "Select the correct option(s)\n",
        "Select the correct option(s) about hyperparameters. (There can be multiple correct options, please select all of them).\n",
        "\n",
        "1/1 XP\n",
        "\n",
        "\n",
        "Learning rate helps us to update the increase or decrease in the values of parameters during gradient descent.\n",
        "\n",
        "Your answer\n",
        "\n",
        "\n",
        "Epochs tell you the number of loops on the training data during building the model.\n",
        "\n",
        "Your answer\n",
        "\n",
        "\n",
        "Batch size is nothing but the number of samples in an epoch used to estimate the model error.\n",
        "\n",
        "Your answer\n",
        "\n",
        "3.\n",
        "Select the correct option(s)\n",
        "Select the correct statement(s), considering you have 100 training examples and you divided it into batches of size 20. (There can be multiple correct options, please select all of them)\n",
        "\n",
        "1/1 XP\n",
        "\n",
        "\n",
        "The number of batches we will have in this case is 5.\n",
        "\n",
        "Your answer\n",
        "\n",
        "\n",
        "The number of batches we will have in this case is 20.\n",
        "\n",
        "\n",
        "When all the batches pass through the model back and forth once, we say it as one epoch.\n",
        "\n",
        "Your answer\n",
        "\n",
        "4.\n",
        "Select the correct option(s)\n",
        "Which of the following rules can help you find the best set of hyperparameters for your dataset? (There can be multiple correct options, please select all of them)\n",
        "\n",
        "1/1 XP\n",
        "\n",
        "\n",
        "Training loss should steadily decrease, steeply at first, and then more slowly until the slope of the curve reaches or approaches zero.\n",
        "\n",
        "Your answer\n",
        "\n",
        "\n",
        "If the training loss does not converge, train for more epochs.\n",
        "\n",
        "Your answer\n",
        "\n",
        "\n",
        "If the training loss decreases too slowly, increase the learning rate. Note that setting the learning rate too high may also prevent training loss from converging.\n",
        "\n",
        "Your answer\n",
        "\n",
        "\n",
        "Lowering the learning rate while increasing the number of epochs or the batch size is often a good combination.\n",
        "\n",
        "Your answer\n",
        "\n",
        "\n",
        "Setting the batch size to a very small batch number can also cause instability. First, try large batch size values. Then, decrease the batch size until you see degradation.\n",
        "\n",
        "Your answer\n",
        "\n",
        "5.\n",
        "Select the correct option\n",
        "Hyperparameter tuning helps us find the optimal learning rate, epochs, and batch sizes.\n",
        "\n",
        "1/1 XP\n",
        "\n",
        "\n",
        "True\n",
        "\n",
        "Your answer\n",
        "\n",
        "\n",
        "False**\n",
        "\n"
      ],
      "metadata": {
        "id": "z3W5GK95cJi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "69mDHOUIxQKV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}