{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mustafabozkaya/Deep_Learning_Bootcamp/blob/master/AI/ML%20Challenge/manually_tuned_densenet201_binary_eye_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYRtpk3xGAeS"
      },
      "source": [
        "**Name : Mustafa Bozkaya**\n",
        "\n",
        "Binary prediction of image: male eye or female eye. Using DenseNet201."
      ],
      "id": "TYRtpk3xGAeS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV75FK1nLaGe"
      },
      "source": [
        "**Download the images**"
      ],
      "id": "AV75FK1nLaGe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU1Auc3m51vj",
        "outputId": "bfe03c40-9b93-4a40-81ac-eba6cf4296fd"
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1f7uslI-ZHidriQFZR966_aILjlkgDN76' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1f7uslI-ZHidriQFZR966_aILjlkgDN76\" -O eye_gender_data.zip && rm -rf /tmp/cookies.txt"
      ],
      "id": "FU1Auc3m51vj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-11 21:09:25--  https://docs.google.com/uc?export=download&confirm=2JLB&id=1f7uslI-ZHidriQFZR966_aILjlkgDN76\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.141.138, 142.250.141.139, 142.250.141.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.141.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0s-c0-docs.googleusercontent.com/docs/securesc/4co1p751tm69k5gf3045cnfpmqenuopt/3mooraeag8q3l6kt9gdam0g4p9n7f2p8/1628716125000/03712597189317496002/11820604109431335396Z/1f7uslI-ZHidriQFZR966_aILjlkgDN76?e=download [following]\n",
            "--2021-08-11 21:09:25--  https://doc-0s-c0-docs.googleusercontent.com/docs/securesc/4co1p751tm69k5gf3045cnfpmqenuopt/3mooraeag8q3l6kt9gdam0g4p9n7f2p8/1628716125000/03712597189317496002/11820604109431335396Z/1f7uslI-ZHidriQFZR966_aILjlkgDN76?e=download\n",
            "Resolving doc-0s-c0-docs.googleusercontent.com (doc-0s-c0-docs.googleusercontent.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84\n",
            "Connecting to doc-0s-c0-docs.googleusercontent.com (doc-0s-c0-docs.googleusercontent.com)|142.251.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=s28h6dcclsbnc&continue=https://doc-0s-c0-docs.googleusercontent.com/docs/securesc/4co1p751tm69k5gf3045cnfpmqenuopt/3mooraeag8q3l6kt9gdam0g4p9n7f2p8/1628716125000/03712597189317496002/11820604109431335396Z/1f7uslI-ZHidriQFZR966_aILjlkgDN76?e%3Ddownload&hash=dl5oo976nqcqsv1hrljsakm1sr7ogjpe [following]\n",
            "--2021-08-11 21:09:25--  https://docs.google.com/nonceSigner?nonce=s28h6dcclsbnc&continue=https://doc-0s-c0-docs.googleusercontent.com/docs/securesc/4co1p751tm69k5gf3045cnfpmqenuopt/3mooraeag8q3l6kt9gdam0g4p9n7f2p8/1628716125000/03712597189317496002/11820604109431335396Z/1f7uslI-ZHidriQFZR966_aILjlkgDN76?e%3Ddownload&hash=dl5oo976nqcqsv1hrljsakm1sr7ogjpe\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.141.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-0s-c0-docs.googleusercontent.com/docs/securesc/4co1p751tm69k5gf3045cnfpmqenuopt/3mooraeag8q3l6kt9gdam0g4p9n7f2p8/1628716125000/03712597189317496002/11820604109431335396Z/1f7uslI-ZHidriQFZR966_aILjlkgDN76?e=download&nonce=s28h6dcclsbnc&user=11820604109431335396Z&hash=ieau7nsnuiqc357bf6d5tgo1nqe9ipjg [following]\n",
            "--2021-08-11 21:09:26--  https://doc-0s-c0-docs.googleusercontent.com/docs/securesc/4co1p751tm69k5gf3045cnfpmqenuopt/3mooraeag8q3l6kt9gdam0g4p9n7f2p8/1628716125000/03712597189317496002/11820604109431335396Z/1f7uslI-ZHidriQFZR966_aILjlkgDN76?e=download&nonce=s28h6dcclsbnc&user=11820604109431335396Z&hash=ieau7nsnuiqc357bf6d5tgo1nqe9ipjg\n",
            "Connecting to doc-0s-c0-docs.googleusercontent.com (doc-0s-c0-docs.googleusercontent.com)|142.251.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-zip-compressed]\n",
            "Saving to: ‘eye_gender_data.zip’\n",
            "\n",
            "eye_gender_data.zip     [   <=>              ]  25.55M  60.1MB/s    in 0.4s    \n",
            "\n",
            "2021-08-11 21:09:26 (60.1 MB/s) - ‘eye_gender_data.zip’ saved [26794203]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CJJufhhLoU9"
      },
      "source": [
        "**Summary of Libraries**"
      ],
      "id": "2CJJufhhLoU9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPLbUd5xLqiy"
      },
      "source": [
        "# import zipfile\n",
        "# import os\n",
        "# import pandas as pd\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# import random\n",
        "# import numpy as np\n",
        "# from sklearn.utils import class_weight\n",
        "# from tensorflow.keras.applications import mobilenet_v2, Xception, NASNetLarge, NASNetMobile, DenseNet201, InceptionResNetV2\n",
        "# from google.colab import files\n",
        "# from keras import backend as K"
      ],
      "id": "qPLbUd5xLqiy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpZ-6pDeMnem"
      },
      "source": [
        "**Loading data**"
      ],
      "id": "lpZ-6pDeMnem"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAL7r95L4kny"
      },
      "source": [
        "import zipfile, os\n",
        "local_zip = '/content/eye_gender_data.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ],
      "id": "WAL7r95L4kny",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba9d2856"
      },
      "source": [
        "img_path = 'eye_gender_data/train/'"
      ],
      "id": "ba9d2856",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c801531"
      },
      "source": [
        "# Pinned this for trial-error someday, maybe\n",
        "# for i in os.listdir(img_path):\n",
        "    # print(i)"
      ],
      "id": "1c801531",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcd0c817",
        "outputId": "4fc13105-5b50-4988-acd2-9a0e776f149f"
      },
      "source": [
        "len(os.listdir(img_path))"
      ],
      "id": "dcd0c817",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9220"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cptxOQroTOh9"
      },
      "source": [
        "**Loading and preparing test data**"
      ],
      "id": "cptxOQroTOh9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CHncKa2srOO"
      },
      "source": [
        "If using imagedatagenerator, photo is sorted alphabetically\n",
        "\n",
        "Test list requirement in Testing_set.csv is sorted ascending, but not alphabetically\n",
        "\n",
        "So the way to prepare photos is either sort test image alphabetically or loop predict based on test's csv"
      ],
      "id": "4CHncKa2srOO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IzInbKh-yvF"
      },
      "source": [
        "test_path = 'eye_gender_data/test/'\n",
        "\n",
        "for i in os.listdir(test_path):\n",
        "  c = 14-len(i)\n",
        "  os.rename(test_path + i, test_path + i[:6] + '0'*c + i[6:])"
      ],
      "id": "2IzInbKh-yvF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk1y5URtU3-V"
      },
      "source": [
        "**Loading and preparing training data**"
      ],
      "id": "jk1y5URtU3-V"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e29f4afd"
      },
      "source": [
        "csv_path = 'eye_gender_data/Training_set.csv'"
      ],
      "id": "e29f4afd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "2b5a586d",
        "outputId": "af5d1e15-2731-4b99-a8e4-28faca5b431f"
      },
      "source": [
        "import pandas as pd\n",
        "d = pd.read_csv(csv_path)\n",
        "d"
      ],
      "id": "2b5a586d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Image_1.jpg</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Image_2.jpg</td>\n",
              "      <td>female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Image_3.jpg</td>\n",
              "      <td>female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Image_4.jpg</td>\n",
              "      <td>female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Image_5.jpg</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9215</th>\n",
              "      <td>Image_9216.jpg</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9216</th>\n",
              "      <td>Image_9217.jpg</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9217</th>\n",
              "      <td>Image_9218.jpg</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9218</th>\n",
              "      <td>Image_9219.jpg</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9219</th>\n",
              "      <td>Image_9220.jpg</td>\n",
              "      <td>female</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9220 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            filename   label\n",
              "0        Image_1.jpg    male\n",
              "1        Image_2.jpg  female\n",
              "2        Image_3.jpg  female\n",
              "3        Image_4.jpg  female\n",
              "4        Image_5.jpg    male\n",
              "...              ...     ...\n",
              "9215  Image_9216.jpg    male\n",
              "9216  Image_9217.jpg    male\n",
              "9217  Image_9218.jpg    male\n",
              "9218  Image_9219.jpg    male\n",
              "9219  Image_9220.jpg  female\n",
              "\n",
              "[9220 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "764d8199"
      },
      "source": [
        "female_path = img_path + 'female/'\n",
        "male_path = img_path + 'male/'"
      ],
      "id": "764d8199",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f5ba78d"
      },
      "source": [
        "# Prepare to use imagedatagenerator: create two folder, male and female and distribute correctly based on Training_set.csv\n",
        "if not os.path.exists(female_path):\n",
        "    os.makedirs(female_path)\n",
        "if not os.path.exists(male_path):\n",
        "    os.makedirs(male_path)\n",
        "\n",
        "for i,j in zip(d.filename, d.label): \n",
        "    try:\n",
        "        os.rename(img_path + i, img_path + j + '/' + i)\n",
        "    except:\n",
        "        pass"
      ],
      "id": "4f5ba78d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eaf0add",
        "outputId": "3c13d0ec-763e-492b-8bba-43fd98ba351d"
      },
      "source": [
        "# A simple distribution check\n",
        "len(os.listdir(male_path))"
      ],
      "id": "8eaf0add",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5058"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8397cc3",
        "outputId": "707d57bd-7044-47a0-8fae-09533d4da84b"
      },
      "source": [
        "len(os.listdir(female_path))"
      ],
      "id": "d8397cc3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4162"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e03bea71"
      },
      "source": [
        "z=len(os.listdir(img_path))"
      ],
      "id": "e03bea71",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZAj0Iu5StuY"
      },
      "source": [
        "# Apply class weight to the model later\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "\n",
        "class_array = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(d.label.values),\n",
        "    y=d.label.values)\n",
        "\n",
        "class_weights = dict(enumerate(class_array))"
      ],
      "id": "0ZAj0Iu5StuY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sF3hk6Wlzks7",
        "outputId": "82e245da-4f8f-42a8-d273-de9b21809a0d"
      },
      "source": [
        "# 0 is female. Since female class is lesser than male, female class get weighted more\n",
        "class_weights"
      ],
      "id": "sF3hk6Wlzks7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 1.1076405574243153, 1: 0.911427441676552}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGeClMTUz1IA",
        "outputId": "2cbe0246-3891-4315-fe87-73adec0de9eb"
      },
      "source": [
        "class_array"
      ],
      "id": "jGeClMTUz1IA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.10764056, 0.91142744])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAE1lRyMRo7n"
      },
      "source": [
        "**Data Pre-processing (on train, validation and test data)**"
      ],
      "id": "nAE1lRyMRo7n"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf8a6b0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65e27475-fc90-4663-8607-d0f22fce7590"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# import random\n",
        "# random_number = random.randint(0, 100)\n",
        "# print(random_number)\n",
        "\n",
        "# import numpy as np\n",
        "# np.random.seed(82)\n",
        "\n",
        "# set_seed = random_number\n",
        "set_seed = 82\n",
        "target_img = (75,75) # Not the smallest but not the largest pixel in the data\n",
        "batch = 256\n",
        "# val_split = 0.02501\n",
        "val_split = 0.0502 # Set fraction fold (here 461 validation image, 20% of test data)\n",
        "\n",
        "# def grayscale_as_rgb_dim(img):\n",
        "#     img = tf.image.rgb_to_grayscale(img)\n",
        "#     img = tf.image.grayscale_to_rgb(img)\n",
        "#     return img\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rescale=1/255.,\n",
        "    # preprocessing_function=grayscale_as_rgb_dim,\n",
        "                    \n",
        "    # featurewise_center=False,\n",
        "    # samplewise_center=False,\n",
        "    # featurewise_std_normalization=False,\n",
        "    # samplewise_std_normalization=False,\n",
        "    )\n",
        "\n",
        "folder_path='eye_gender_data/'\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    folder_path,\n",
        "    # only read images from test directory\n",
        "    classes=['test'],\n",
        "    # color_mode='grayscale',\n",
        "    # don't generate labels\n",
        "    class_mode=None,\n",
        "    shuffle=False,\n",
        "    target_size=target_img\n",
        "    )\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1/255,\n",
        "    rotation_range=10,\n",
        "    horizontal_flip=True,\n",
        "    # vertical_flip=True,\n",
        "    width_shift_range=0.125,\n",
        "    height_shift_range=[-0.15, 0.05],\n",
        "    # shear_range = 0.2,\n",
        "    zoom_range=0.1,\n",
        "    # brightness_range=[0.9,1.0],\n",
        "    fill_mode = 'nearest',\n",
        "    validation_split=val_split,\n",
        "    # preprocessing_function=grayscale_as_rgb_dim,\n",
        "\n",
        "    # featurewise_center=False,\n",
        "    # samplewise_center=False,\n",
        "    # featurewise_std_normalization=False,\n",
        "    # samplewise_std_normalization=False,\n",
        "    )\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    img_path,\n",
        "    target_size=target_img,\n",
        "    # color_mode='grayscale',\n",
        "    batch_size=batch,\n",
        "    class_mode='binary',\n",
        "    subset='training',\n",
        "    seed=set_seed\n",
        "    )\n",
        "\n",
        "datagenval = ImageDataGenerator(\n",
        "    rescale=1/255,\n",
        "    validation_split=val_split,\n",
        "    # preprocessing_function=grayscale_as_rgb_dim,\n",
        "    \n",
        "    # featurewise_center=False,\n",
        "    # samplewise_center=False,\n",
        "    # featurewise_std_normalization=False,\n",
        "    # samplewise_std_normalization=False,\n",
        "    )\n",
        "\n",
        "validation_generator = datagenval.flow_from_directory(\n",
        "    img_path,\n",
        "    target_size=target_img,\n",
        "    # color_mode='grayscale',\n",
        "    batch_size=batch,\n",
        "    class_mode='binary',\n",
        "    subset='validation',\n",
        "    seed=set_seed\n",
        "    )"
      ],
      "id": "bf8a6b0d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2305 images belonging to 1 classes.\n",
            "Found 8759 images belonging to 2 classes.\n",
            "Found 461 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RExzYWPRR39k"
      },
      "source": [
        "**Building Model & Hyperparameter tuning (Manually)**"
      ],
      "id": "RExzYWPRR39k"
    },
    {
      "cell_type": "code",
      "metadata": {
        "run_control": {
          "marked": false
        },
        "id": "ff5ed814"
      },
      "source": [
        "from tensorflow.keras.applications import mobilenet_v2, Xception, NASNetLarge, NASNetMobile, DenseNet201, InceptionResNetV2\n",
        "\n",
        "tf.random.set_seed(set_seed)\n",
        "model = tf.keras.models.Sequential([\n",
        "#   tf.keras.layers.InputLayer((*target_img,3)),\n",
        "#   tf.keras.layers.Conv2D(16, (3,3), activation='relu', padding='same'),\n",
        "#   tf.keras.layers.BatchNormalization(),\n",
        "#   tf.keras.layers.MaxPooling2D(2, 2),\n",
        "# #   # tf.keras.layers.Dropout(0.9),\n",
        "#   tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding='same'),\n",
        "#   tf.keras.layers.BatchNormalization(),\n",
        "#   tf.keras.layers.MaxPooling2D(2,2),\n",
        "# #   tf.keras.layers.Dropout(0.9),\n",
        "#   tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "#   tf.keras.layers.BatchNormalization(),\n",
        "#   tf.keras.layers.MaxPooling2D(2,2),\n",
        "# #   tf.keras.layers.Dropout(0.9),\n",
        "#   tf.keras.layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n",
        "#   tf.keras.layers.BatchNormalization(),\n",
        "#   tf.keras.layers.MaxPooling2D(2,2),\n",
        "# #   tf.keras.layers.Dropout(0.9),\n",
        "#   tf.keras.layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n",
        "#   tf.keras.layers.BatchNormalization(),\n",
        "#   tf.keras.layers.MaxPooling2D(2,2),\n",
        "#   # tf.keras.layers.Dropout(0.9),\n",
        "#   tf.keras.layers.Conv2D(512, (3,3), activation='relu', padding='same'),\n",
        "#   tf.keras.layers.BatchNormalization(),\n",
        "#   tf.keras.layers.MaxPooling2D(2,2),\n",
        "#   # tf.keras.layers.Dropout(0.9),\n",
        "    \n",
        "#   tf.keras.layers.Conv2D(1024, (3,3), activation='relu', padding='same'),\n",
        "#   tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "  # InceptionResNetV2(weights=\"imagenet\", include_top=False, input_shape=(*target_img,3)), # Minimal must 75x75 \n",
        "  DenseNet201(weights=\"imagenet\", include_top=False, input_shape=(*target_img,3)),\n",
        "  # NASNetMobile(weights=\"imagenet\", include_top=False, input_shape=(*target_img,3)), # Must 224x224\n",
        "  # NASNetLarge(weights=\"imagenet\", include_top=False, input_shape=(*target_img,3)), # Must 224x224\n",
        "  # Xception(weights=\"imagenet\", include_top=False, input_shape=(*target_img,3)), # Minimal must 71x71 \n",
        "  # mobilenet_v2.MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(*target_img,3)),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(512, activation='relu'),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  # tf.keras.layers.Dropout(0.95),\n",
        "#   tf.keras.layers.GlobalAveragePooling2D(),\n",
        "  tf.keras.layers.Dense(z-1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# model.layers[0].trainable = True"
      ],
      "id": "ff5ed814",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks8DFQmIv4u2"
      },
      "source": [
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.SGD(learning_rate=5e-2),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "id": "Ks8DFQmIv4u2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa337850",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4151474a-94f5-400f-f993-95c2d5dcfc02"
      },
      "source": [
        "model.summary()"
      ],
      "id": "aa337850",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "densenet201 (Functional)     (None, 2, 2, 1920)        18321984  \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 2, 2, 1920)        7680      \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 7680)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 512)               3932672   \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 22,264,897\n",
            "Trainable params: 22,030,977\n",
            "Non-trainable params: 233,920\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8795705"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if(logs.get('val_accuracy')==1):\n",
        "            print(\"\\nMaximum Validtion Accuracy has reached!\")\n",
        "            self.model.stop_training = True\n",
        "callback = myCallback()\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    'model5.h5',\n",
        "    monitor = 'val_accuracy',\n",
        "    save_best_only = True,\n",
        "    verbose = 1\n",
        ")\n",
        "\n",
        "# reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "#     monitor = 'val_accuracy',\n",
        "#     patience = 15,\n",
        "#     factor = 0.02,\n",
        "#     min_lr = 1e-5,\n",
        "#     # cooldown = 5,\n",
        "#     verbose = 1\n",
        "# )\n",
        "\n",
        "set_callback = [\n",
        "  checkpoint, \n",
        "  # reduce_lr,\n",
        "  # callback,\n",
        "  # early\n",
        "]"
      ],
      "id": "c8795705",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmAvV7ZjWVto"
      },
      "source": [
        "**Load model**"
      ],
      "id": "gmAvV7ZjWVto"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjWcCQjPS1Qi"
      },
      "source": [
        "# model.load_weights('model5.h5')\n",
        "# model.load_weights('check9.h5')\n",
        "# print(K.eval(model.optimizer.lr)) # Print learning rate of model"
      ],
      "id": "jjWcCQjPS1Qi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqGecTqwWbCT"
      },
      "source": [
        "**Change learning rate amid fit**"
      ],
      "id": "OqGecTqwWbCT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjQL8y2Ga3m3"
      },
      "source": [
        "# How to use: set smaller when we feel the fit begin to falls on the right minima (global)\n",
        "\n",
        "# K.set_value(model.optimizer.learning_rate, 1e-3) # Set new learning rate for model\n",
        "# print(K.eval(model.optimizer.lr))"
      ],
      "id": "qjQL8y2Ga3m3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM9CT44ZTAm7"
      },
      "source": [
        "**Validate the model**"
      ],
      "id": "tM9CT44ZTAm7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB9DpII542ad"
      },
      "source": [
        "Usually I manually run and stop amid the fit like this. This actually has been going to more than 100 epoch."
      ],
      "id": "tB9DpII542ad"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVWMwK4jVyde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf8bbb19-e0c8-42ac-99c5-935a9c9c136e"
      },
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      epochs = 30,\n",
        "      validation_data=validation_generator,\n",
        "      callbacks=[set_callback],\n",
        "      class_weight=class_weights,\n",
        "      verbose = 1\n",
        "      )"
      ],
      "id": "NVWMwK4jVyde",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "35/35 [==============================] - 23s 655ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.1111 - val_accuracy: 0.9761\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.98698\n",
            "Epoch 2/50\n",
            "35/35 [==============================] - 24s 684ms/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 0.0942 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.98698\n",
            "Epoch 3/50\n",
            "35/35 [==============================] - 23s 656ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.1334 - val_accuracy: 0.9740\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.98698\n",
            "Epoch 4/50\n",
            "35/35 [==============================] - 24s 663ms/step - loss: 0.0085 - accuracy: 0.9973 - val_loss: 0.1160 - val_accuracy: 0.9588\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.98698\n",
            "Epoch 5/50\n",
            "35/35 [==============================] - 24s 683ms/step - loss: 0.0048 - accuracy: 0.9984 - val_loss: 0.1298 - val_accuracy: 0.9718\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.98698\n",
            "Epoch 6/50\n",
            "35/35 [==============================] - 23s 650ms/step - loss: 0.0053 - accuracy: 0.9981 - val_loss: 0.1061 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.98698\n",
            "Epoch 7/50\n",
            "35/35 [==============================] - 24s 684ms/step - loss: 0.0021 - accuracy: 0.9992 - val_loss: 0.1168 - val_accuracy: 0.9761\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.98698\n",
            "Epoch 8/50\n",
            "35/35 [==============================] - 24s 662ms/step - loss: 0.0034 - accuracy: 0.9989 - val_loss: 0.1077 - val_accuracy: 0.9740\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.98698\n",
            "Epoch 9/50\n",
            "35/35 [==============================] - 23s 654ms/step - loss: 0.0038 - accuracy: 0.9986 - val_loss: 0.1490 - val_accuracy: 0.9653\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.98698\n",
            "Epoch 10/50\n",
            "35/35 [==============================] - 23s 660ms/step - loss: 0.0020 - accuracy: 0.9992 - val_loss: 0.0999 - val_accuracy: 0.9783\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.98698\n",
            "Epoch 11/50\n",
            "35/35 [==============================] - 23s 651ms/step - loss: 0.0035 - accuracy: 0.9991 - val_loss: 0.0941 - val_accuracy: 0.9761\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.98698\n",
            "Epoch 12/50\n",
            "35/35 [==============================] - 24s 662ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.0870 - val_accuracy: 0.9740\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.98698\n",
            "Epoch 13/50\n",
            "35/35 [==============================] - 24s 671ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0904 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.98698\n",
            "Epoch 14/50\n",
            "35/35 [==============================] - 23s 639ms/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 0.0973 - val_accuracy: 0.9761\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.98698\n",
            "Epoch 15/50\n",
            "35/35 [==============================] - 24s 664ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.1045 - val_accuracy: 0.9783\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.98698\n",
            "Epoch 16/50\n",
            "35/35 [==============================] - 23s 638ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.0878 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.98698\n",
            "Epoch 17/50\n",
            "35/35 [==============================] - 24s 683ms/step - loss: 0.0044 - accuracy: 0.9989 - val_loss: 0.1476 - val_accuracy: 0.9588\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.98698\n",
            "Epoch 18/50\n",
            "35/35 [==============================] - 24s 677ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 0.1147 - val_accuracy: 0.9761\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.98698\n",
            "Epoch 19/50\n",
            "35/35 [==============================] - 23s 648ms/step - loss: 0.0020 - accuracy: 0.9992 - val_loss: 0.1000 - val_accuracy: 0.9761\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.98698\n",
            "Epoch 20/50\n",
            "35/35 [==============================] - 24s 673ms/step - loss: 0.0016 - accuracy: 0.9994 - val_loss: 0.0944 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.98698\n",
            "Epoch 21/50\n",
            "35/35 [==============================] - 23s 648ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.1423 - val_accuracy: 0.9740\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.98698\n",
            "Epoch 22/50\n",
            "35/35 [==============================] - 24s 670ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.1339 - val_accuracy: 0.9783\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.98698\n",
            "Epoch 23/50\n",
            " 5/35 [===>..........................] - ETA: 19s - loss: 0.0036 - accuracy: 0.9984"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-247-852b7ede6b1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mset_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m       \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1186\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \"\"\"\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    315\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    335\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    513\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \"\"\"\n\u001b[1;32m   1093\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1058\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOogOAV5sb9L"
      },
      "source": [
        "Using GPU we will never get the same result. Actually all the seed here is futile (except imagedatagenerator seed)\n",
        "\n",
        "So in competition, when we get a good result in submission, don't forget to save\n",
        "\n",
        "Then we can load it (architecture must be same, some imagedatagenerator properties must same)\n",
        "\n",
        "Actually we can do without model.fit at first at all"
      ],
      "id": "IOogOAV5sb9L"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnaVeaKOSOvU"
      },
      "source": [
        "**Make Prediction on Test Dataset and Save**"
      ],
      "id": "BnaVeaKOSOvU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FhR0l1YwF8T"
      },
      "source": [
        "preds = model.predict(test_generator)\n",
        "preds;"
      ],
      "id": "4FhR0l1YwF8T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUdjRK9vmiBG"
      },
      "source": [
        "a=preds.round()\n",
        "a=pd.DataFrame(a, columns=['label'])\n",
        "a.label=a.label.map(lambda x: 'female' if x==0 else 'male')\n",
        "a.to_csv('first.csv', index=False)\n",
        "a;"
      ],
      "id": "AUdjRK9vmiBG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD9BRG-LoA0V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "89701138-6753-46ed-c6a6-eae2ee839da4"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('first.csv') "
      ],
      "id": "TD9BRG-LoA0V",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_d8636320-ed68-49ac-8696-3166cf592d92\", \"first.csv\", 13511)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6-c7Q0SlgC7"
      },
      "source": [
        "# If manually tune, maybe we want to stop amid epoch process and want to check validation accuracy\n",
        "model.evaluate(validation_generator)"
      ],
      "id": "O6-c7Q0SlgC7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXP0nOiOWNn7"
      },
      "source": [
        "**Save model and/or download locally**"
      ],
      "id": "rXP0nOiOWNn7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV07c6j1R8N0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "5bf2ecd8-ea0b-43b4-8894-51833926d5fb"
      },
      "source": [
        "# model.save('check9.h5')\n",
        "# files.download('check9.h5') "
      ],
      "id": "SV07c6j1R8N0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_933983b3-b4ec-4d0b-ba8f-73d6c077d8d0\", \"check9.h5\", 121724208)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBIHtyi0FSl-"
      },
      "source": [
        "# files.download('model5.h5') "
      ],
      "id": "RBIHtyi0FSl-",
      "execution_count": null,
      "outputs": []
    }
  ]
}